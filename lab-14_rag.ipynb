{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AdvNLP Lab (Graded Lab): Experimenting with Retrieval as Part of a RAG System\n",
    "\n",
    "Total: 44 points\n",
    "\n",
    "**Objectives:** We build the retrieval part of a RAG system and compare performance of classic KNN retrieval with additional cross encoder reranking. Eventually, we write two prompts for generation and test it on a LLM.\n",
    "\n",
    "**Useful documentation:** Since you'll use LangChain for this assignment, [their documentation](https://python.langchain.com/docs/introduction/) might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "Künzi Dominic, Matzinger Jaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to install the required packages for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pandas langchain-community langchain-huggingface faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import pyperclip\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [DRAGONBall Dataset](https://github.com/OpenBMB/RAGEval) as a basis for this assignment and load a subset of their documents. These will be the stored knowledge of the RAG system. To store them into the vector store, we will later directly create embeddings out of them, since they have alredy the size of suitable chunks. Each document consists of a unique ID and the actual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Acme Government Solutions is a government indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Entertainment Enterprises Inc. is an entertain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Advanced Manufacturing Solutions Inc., establi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>EcoGuard Solutions, established on April 15, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Green Fields Agriculture Ltd., established on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Hospitalization Record:\\n\\nBasic Information:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>**Hospitalization Record**\\n\\n**Basic Informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Hospitalization Record\\n\\nBasic Information:\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Hospitalization Record\\n----------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Hospitalization Record:\\n\\nBasic Information:\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content\n",
       "id                                                    \n",
       "40   Acme Government Solutions is a government indu...\n",
       "41   Entertainment Enterprises Inc. is an entertain...\n",
       "42   Advanced Manufacturing Solutions Inc., establi...\n",
       "43   EcoGuard Solutions, established on April 15, 2...\n",
       "44   Green Fields Agriculture Ltd., established on ...\n",
       "..                                                 ...\n",
       "211  Hospitalization Record:\\n\\nBasic Information:\\...\n",
       "212  **Hospitalization Record**\\n\\n**Basic Informat...\n",
       "213  Hospitalization Record\\n\\nBasic Information:\\n...\n",
       "214  Hospitalization Record\\n----------------------...\n",
       "215  Hospitalization Record:\\n\\nBasic Information:\\...\n",
       "\n",
       "[108 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.read_csv('./data/docs.csv', index_col=0)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of the assignment is to evaluate the retrieval component of the RAG system. For that, we also load a dataset of queries, which we can use to retrieve matching documents. Each query has also assigned an array of documents in the form of their IDs, which match with the documents loaded before. We can use these to evaluate whether the correct documents were found by the retrieval or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth_doc_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>When was Sparkling Clean Housekeeping Services...</td>\n",
       "      <td>[64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>How did HealthPro Innovations' strategic partn...</td>\n",
       "      <td>[54]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>According to the hospitalization records of Br...</td>\n",
       "      <td>[212]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>According to the judgment of Norwood, Unionvil...</td>\n",
       "      <td>[124]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>Based on HealthLife Solutions' 2020 corporate ...</td>\n",
       "      <td>[73]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>How did the severe drought in August 2018 lead...</td>\n",
       "      <td>[65]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>Compare the large-scale financing activities o...</td>\n",
       "      <td>[58, 55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>How did CleanCo Housekeeping Services' investm...</td>\n",
       "      <td>[47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>What were the outcomes of the debt restructuri...</td>\n",
       "      <td>[56, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>According to the hospitalization records of Wi...</td>\n",
       "      <td>[188, 213]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      query  \\\n",
       "query_id                                                      \n",
       "2286      When was Sparkling Clean Housekeeping Services...   \n",
       "2433      How did HealthPro Innovations' strategic partn...   \n",
       "6266      According to the hospitalization records of Br...   \n",
       "4499      According to the judgment of Norwood, Unionvil...   \n",
       "2448      Based on HealthLife Solutions' 2020 corporate ...   \n",
       "...                                                     ...   \n",
       "2186      How did the severe drought in August 2018 lead...   \n",
       "3251      Compare the large-scale financing activities o...   \n",
       "2268      How did CleanCo Housekeeping Services' investm...   \n",
       "3311      What were the outcomes of the debt restructuri...   \n",
       "6551      According to the hospitalization records of Wi...   \n",
       "\n",
       "         ground_truth_doc_ids  \n",
       "query_id                       \n",
       "2286                     [64]  \n",
       "2433                     [54]  \n",
       "6266                    [212]  \n",
       "4499                    [124]  \n",
       "2448                     [73]  \n",
       "...                       ...  \n",
       "2186                     [65]  \n",
       "3251                 [58, 55]  \n",
       "2268                     [47]  \n",
       "3311                 [56, 53]  \n",
       "6551               [188, 213]  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_csv('./data/queries.csv', index_col=0)\n",
    "queries['ground_truth_doc_ids'] = queries['ground_truth_doc_ids'].apply(lambda x: x.split(';'))\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recall@N\n",
    "\n",
    "**1a) [2 points]** We will evaluate the retrieval by comparing the retrieved documents with the ground truth documents assigned to the query. For that, we will use the Recall@N metric. Please describe in 1-2 sentences how we can interpret this metric in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "The higher the recall, the more documents from the ground truth were found in the top N positions of the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b) [4 points]** Implement the Recall@N metric and test it with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_n(retrieved_docs, relevant_doc_ids, n):\n",
    "    \"\"\"\n",
    "    Calculate Recall@N.\n",
    "\n",
    "    Parameters:\n",
    "    - retrieved_docs: Sorted list of retrieved documents as LangChain Document objects\n",
    "    - relevant_doc_ids: List of relevant document IDs\n",
    "    - n: Number of top documents to consider\n",
    "\n",
    "    Returns:\n",
    "    - Recall@N\n",
    "    \"\"\"\n",
    "    # TODO YOUR CODE HERE\n",
    "    top_n_docs = retrieved_docs[:n]\n",
    "    top_n_ids = [doc.metadata['id'] for doc in top_n_docs]\n",
    "\n",
    "    relevant_in_top_n = sum(1 for doc_id in top_n_ids if doc_id in relevant_doc_ids)\n",
    "\n",
    "    if not relevant_doc_ids:\n",
    "        return 0.0\n",
    "\n",
    "    return relevant_in_top_n / len(relevant_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test\n",
    "\n",
    "recall_at_n(\n",
    "    [Document(page_content='', metadata={'id': str(id)}) for id in range(10)],\n",
    "    ['0', '1', '20'],\n",
    "    3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Model\n",
    "\n",
    "**2a) [3 points]** Each document will be converted to an embedding representing the semantic meaning of the document. In this assignment, we will use model `sentence-transformers/all-MiniLM-L6-v2` from HuggingFace. Please answer the following questions about this model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers:**\n",
    "\n",
    "Embedding Length: 384\n",
    "\n",
    "Number of Parameters: 22.7 Million\n",
    "\n",
    "Maximum Sequence Length: 256 word pieces (during training, they limited it to 128 tokens, as stated on the HF model card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Store\n",
    "\n",
    "**3a) [4 points]** Use LangChain to create a FAISS vector store and embed the documents with the above-mentioned embedding model. Load the documents again but this time with a Loader object from LangChain. Eventually, print the number of documents in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of docs in the vector store: 108\n"
     ]
    }
   ],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "loader = CSVLoader(file_path='./data/docs.csv', source_column='content', metadata_columns=['id'])\n",
    "documents = loader.load()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "print(f'Nr. of docs in the vector store: {len(vectorstore.index_to_docstore_id)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b) [3 points]** Retrieve the Top-3 documents for this query: \"According to the hospitalization records of Bridgewater General Hospital, summarize the present illness of J. Reyes.\" and print the documents' ID and L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 matching documents:\n",
      "ID: 212, L2 Distance: 0.7302\n",
      "ID: 213, L2 Distance: 0.9898\n",
      "ID: 210, L2 Distance: 1.0050\n"
     ]
    }
   ],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "query = 'According to the hospitalization records of Bridgewater General Hospital, summarize the present illness of J. Reyes.'\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print('Top-3 matching documents:')\n",
    "for doc, score in results:\n",
    "    doc_id = doc.metadata.get('id', 'N/A')\n",
    "    print(f'ID: {doc_id}, L2 Distance: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c) [2 points]** Check and show if a suitable document is found for the query in the Top-3 retrieved documents and show the relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 matching documents:\n",
      "Doc ID: 212\n",
      "content: **Hospitalization Record**\n",
      "\n",
      "**Basic Information:**\n",
      "Name: J. Reyes\n",
      "Gender: Male\n",
      "Age: 52\n",
      "Ethnicity: Hispanic\n",
      "Marital Status: Married\n",
      "Occupation: Construction Worker\n",
      "Address: 22, Sunnyvale street, Bridgewater\n",
      "Admission Time: 7th, September\n",
      "Record Time: 8th, September\n",
      "Historian: Self\n",
      "Hospital Name: Bridgewater General Hospital\n",
      "\n",
      "**Chief Complai...\n",
      "----------------------------------------\n",
      "Doc ID: 213\n",
      "content: Hospitalization Record\n",
      "\n",
      "Basic Information:\n",
      "Name: K. Ramos\n",
      "Gender: Female\n",
      "Age: 82\n",
      "Ethnicity: Caucasian\n",
      "Marital Status: Widowed\n",
      "Occupation: Retired\n",
      "Address: 21, Greenfield Street, Windsor\n",
      "Admission Time: 26th October\n",
      "Record Time: 26th October, 10:00 AM\n",
      "Historian: S. Martinez, MD\n",
      "Hospital Name: Windsor General Hospital\n",
      "\n",
      "Chief Complaint:\n",
      "Sever...\n",
      "----------------------------------------\n",
      "Doc ID: 210\n",
      "content: Hospitalization Record\n",
      "\n",
      "Basic Information:\n",
      "Name: J. Alvarez\n",
      "Gender: Female\n",
      "Age: 83\n",
      "Ethnicity: Hispanic\n",
      "Marital Status: Widowed\n",
      "Occupation: Retired\n",
      "Address: 23, Yarmouth Street, Clearwater\n",
      "Admission Time: 21st, August\n",
      "Record Time: 21st, August\n",
      "Historian: Self\n",
      "Hospital Name: Clearwater General Hospital\n",
      "\n",
      "Chief Complaint:\n",
      "Repeated respiratory ...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "for doc, score in results:\n",
    "    print(f'Doc ID: {doc.metadata.get(\"id\", \"N/A\")}')\n",
    "    print(f'{doc.page_content[:350]}...')\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "According to the L2-Distance, the most similar document contains the hospitalization record of J. Reyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Store Evaluation\n",
    "\n",
    "**4a) [4 points]** Now, we will search with each of the queries for the most relevant documents in the vector store, and calculate Recall@N with them and the assigned ground truth document IDs. To aggregate the results over all queries, we will calculate the mean. We will do this 3 times to and use a different value for $N$ each time: $N \\in \\{ 1, 3, 5, 25\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall@N across all queries:\n",
      "Recall@1: 0.6650\n",
      "Recall@3: 0.8150\n",
      "Recall@5: 0.8600\n",
      "Recall@25: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "N = [1, 3, 5, 25]\n",
    "recall_scores = {n: [] for n in N}\n",
    "\n",
    "for _, row in queries.iterrows():\n",
    "    query = row['query']\n",
    "    try:\n",
    "        relevant_doc_ids = row['ground_truth_doc_ids']\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for n in N:\n",
    "        results = vectorstore.similarity_search(query, k=n)\n",
    "\n",
    "        recall = recall_at_n(results, relevant_doc_ids, n)\n",
    "        recall_scores[n].append(recall)\n",
    "\n",
    "print('Mean Recall@N across all queries:')\n",
    "for n in N:\n",
    "    if recall_scores[n]:\n",
    "        avg_recall = sum(recall_scores[n]) / len(recall_scores[n])\n",
    "        print(f'Recall@{n}: {avg_recall:.4f}')\n",
    "    else:\n",
    "        print(f'Recall@{n}: No valid queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b) [2 points]** When looking at the four calculated Recall@N scores, what do you observe and how can you explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "As more documents are considered the chance of including a relevant document goes up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross Encoder\n",
    "\n",
    "**5a) [3 points]** We want to use a cross encoder model to rerank the retrieved documents. Describe in 1-2 sentences how a new document order can be determined using a cross encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "A cross encoder reranks retrieved documents by jointly encoding query-document pairs and scoring their relevance using a classification or regression head (e.g. similarity score). By computing scores for each pair, the encoder can reorder the documents so that those most semantically aligned with the query are ranked highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b) [4 points]** Now again, we want to calculate Recall@N for all queries and the same $N$ as before. This time, we want to rerank the Top-25 retrieved documents using the cross encoder model `BAAI/bge-reranker-base`. Implement this using LangChain components and report the average Recall for $N \\in \\{ 1, 3, 5, 25\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall@N after reranking with Cross-Encoder:\n",
      "Recall@1: 0.7400\n",
      "Recall@3: 0.9600\n",
      "Recall@5: 0.9800\n",
      "Recall@25: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "cross_encoder = HuggingFaceCrossEncoder(model_name='BAAI/bge-reranker-base')\n",
    "reranker = CrossEncoderReranker(model=cross_encoder, top_n=25)\n",
    "\n",
    "N = [1, 3, 5, 25]\n",
    "recall_scores = {n: [] for n in N}\n",
    "\n",
    "for _, row in queries.iterrows():\n",
    "    query = row['query']\n",
    "    try:\n",
    "        relevant_doc_ids = row['ground_truth_doc_ids']\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    initial_results = vectorstore.similarity_search(query, k=25)\n",
    "    reranked_results = reranker.compress_documents(initial_results, query)\n",
    "\n",
    "    for n in N:\n",
    "        recall = recall_at_n(reranked_results, relevant_doc_ids, n)\n",
    "        recall_scores[n].append(recall)\n",
    "\n",
    "print('Mean Recall@N after reranking with Cross-Encoder:')\n",
    "for n in N:\n",
    "    if recall_scores[n]:\n",
    "        avg_recall = sum(recall_scores[n]) / len(recall_scores[n])\n",
    "        print(f'Recall@{n}: {avg_recall:.4f}')\n",
    "    else:\n",
    "        print(f'Recall@{n}: No valid queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c) [2 points]** What do you observe when you compare the Recall@N scores after reranking with the scores without reranking? Write 1-2 sentences about this and why this might happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "After reranking with the Cross-Encoder, the Recall@N improved as follows:\n",
    "- Recall@1 from 0.665 to 0.740\n",
    "- Recall@3 from 0.815 to 0.960\n",
    "- Recall@5 from 0.860 to 0.980\n",
    "\n",
    "\n",
    "This improvement occurs because the Cross-Encoder performs deep semantic comparison between the query and documents, allowing it to more accurately rank the most relevant documents at the top, which dense vector similarity alone might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation\n",
    "\n",
    "**6a) [6 points]** After improving the retrieval part of the RAG system, we want to finally generate an answer for our query. Retrieve the most relevant document for query \"How much funding did HealthPro Innovations raise in February 2021?\" and print its ID. Then write the instruction message of a prompt to answer this query including all necessary elements before running it using your favourite LLM (ChatGPT GPT-4o, etc.). Please paste the answer from the model and indicate which model you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant document ID: 54\n",
      "Page content has been successfully copied to the clipboard.\n"
     ]
    }
   ],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "query = 'How much funding did HealthPro Innovations raise in Febuary 2021?'\n",
    "\n",
    "initial_results = vectorstore.similarity_search(query, k=25)\n",
    "reranked_results = reranker.compress_documents(initial_results, query)\n",
    "\n",
    "top_doc = reranked_results[0]\n",
    "doc_id = top_doc.metadata.get('id', 'N/A')\n",
    "print(f'Most relevant document ID: {doc_id}')\n",
    "\n",
    "try:\n",
    "    pyperclip.copy(top_doc.page_content)\n",
    "    print('Page content has been successfully copied to the clipboard.')\n",
    "except:\n",
    "    print('Unable to copy page content to the clipboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Prompt:**\n",
    "\n",
    "You are an expert financial analyst assistant. Based on the following document, answer the user's question with a direct and concise response. If the answer is not clearly found in the document, reply with 'Information not available in the provided document.'\n",
    "\n",
    "Question: How much funding did HealthPro Innovations raise in February 2021?\n",
    "\n",
    "Document:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generated Answer:**\n",
    "HealthPro Innovations raised \\$150 million in February 2021.\n",
    "\n",
    "**Used Model:**\n",
    "GPT-4o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6b) [3 points]** We want to use in-context learning and provide the LLM one example of a possible answer. Use the same prompt and extend it, that it should follow this example answer: \"Yep, they sold a lot in that year. Over 50 million units as I can see — pretty big move, respect!\". Use the same model, create a fresh chat and run this new prompt. Highlight the changes in the prompt using **bold style** or <span style=\"color:red;\">color</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Prompt:**\n",
    "\n",
    "You are an expert financial analyst assistant. Based on the following document, answer the user's question with a direct and concise response. If the answer is not clearly found in the document, reply with \"Information not available in the provided document.\"\n",
    "\n",
    "**Here is an example of how to answer:**\n",
    "\n",
    "**Q: How many units did the company sell last year?**\n",
    "\n",
    "**A: Yep, they sold a lot in that year. Over 50 million units as I can see — pretty big move, respect!**\n",
    "\n",
    "**Now use the same tone and style to answer the following.**\n",
    "\n",
    "Question: How much funding did HealthPro Innovations raise in February 2021?\n",
    "\n",
    "Document:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generated Answer:** \n",
    "They pulled in a solid \\$150 million in February 2021 — big-time funding move right there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6c) [2 points]** Please check if the two answers are correct according to the document and how they differ. Does the model follow the example in the second prompt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "1. Both answers are correct according to the document\n",
    "2. The first answer is more formal and neutral, focusing on the facts. In contrast, the second answer is more casual and mimics the tone of the example\n",
    "3. Yes, the model clearly followed the tone and instruction from the example in the second prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of AdvNLP Lab\n",
    "\n",
    "Please make sure all cells have been executed, save this completed notebook, and upload it to Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
